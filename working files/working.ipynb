{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2287cc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from os.path import isfile, join\n",
    "import numpy as np\n",
    "import shutil\n",
    "from tensorflow import keras\n",
    "from pathlib import Path\n",
    "from IPython.display import display, Audio\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5d5d61d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.load_model('models/train.h5')\n",
    "\n",
    "def predict(path, labels):\n",
    "    test = paths_and_labels_to_dataset(path, labels)\n",
    "    \n",
    "    test = test.shuffle(buffer_size = batch_size * 8, seed = shuffle_seed).batch(batch_size)\n",
    "    test = test.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    \n",
    "    test = test.map(lambda x, y: (add_noise(x, noises, scale = scale), y))\n",
    "    \n",
    "    for audios, labels in test.take(1):\n",
    "        ffts = audio_to_fft(audios)\n",
    "        y_pred = model.predict(ffts)\n",
    "        rnd = np.random.randint(0, 1, 1)\n",
    "        audios = audios.numpy()[rnd, :]\n",
    "        labels = labels.numpy()[rnd]\n",
    "        y_pred = np.argmax(y_pred, axis = -1)[rnd]\n",
    "        \n",
    "    for index in range(1):\n",
    "        print(\n",
    "            \"Speaker:\\33{} {}\\33[0m\\tPredicted:\\33{} {}\\33[0m\".format(\n",
    "                \"[92m\", y_pred[index],\n",
    "                \"[92m\", y_pred[index]\n",
    "            )\n",
    "        )\n",
    "        print(\"Speaker Predicted : \", class_names[y_pred[index]])\n",
    "        \n",
    "    return class_names[y_pred[index]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6dc14def",
   "metadata": {},
   "outputs": [],
   "source": [
    "def paths_and_labels_to_dataset(audio_paths, labels):\n",
    "    path_ds = tf.data.Dataset.from_tensor_slices(audio_paths)\n",
    "    audio_ds = path_ds.map(lambda x: path_to_audio(x))\n",
    "    label_ds = tf.data.Dataset.from_tensor_slices(labels)\n",
    "    return tf.data.Dataset.zip((audio_ds, label_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "690dcbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def path_to_audio(path):\n",
    "    audio = tf.io.read_file(path)\n",
    "    audio, _ = tf.audio.decode_wav(audio, 1, sample_rate)\n",
    "    return audio\n",
    "\n",
    "sample_rate = 16000\n",
    "valid_split = 0.1\n",
    "shuffle_seed = 43\n",
    "scale = 0.5\n",
    "batch_size = 128\n",
    "epochs = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1115b78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dictionary = \"data\"\n",
    "audio_folder = \"audio\"\n",
    "noise_folder = \"noise\"\n",
    "\n",
    "audio_path = os.path.join(data_dictionary, audio_folder)\n",
    "noise_path = os.path.join(data_dictionary, noise_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17c39839",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_paths = []\n",
    "for subdir in os.listdir(noise_path):\n",
    "    subdir_path = Path(noise_path) / subdir\n",
    "    if os.path.isdir(subdir_path):\n",
    "        noise_paths +=[\n",
    "            os.path.join(subdir_path, filepath)\n",
    "            for filepath in os.listdir(subdir_path)\n",
    "            if filepath.endswith(\".wav\")\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e232ea45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fem_speaker', 'axb_speaker', 'vishnu', 'gka_speaker', 'slp_speaker', 'ljm_speaker', 'ahw_speaker', 'eey_speaker']\n",
      "Speaker:  fem_speaker\n",
      "Speaker:  axb_speaker\n",
      "Speaker:  vishnu\n",
      "Speaker:  gka_speaker\n",
      "Speaker:  slp_speaker\n",
      "Speaker:  ljm_speaker\n",
      "Speaker:  ahw_speaker\n",
      "Speaker:  eey_speaker\n"
     ]
    }
   ],
   "source": [
    "class_names = os.listdir(audio_path)\n",
    "class_names.remove('.DS_Store')\n",
    "print(class_names, )\n",
    "\n",
    "audio_paths = []\n",
    "labels = []\n",
    "for label, name in enumerate(class_names):\n",
    "    print(\"Speaker: \", (name))\n",
    "    dir_path = Path(audio_path) / name\n",
    "    speaker_sample_paths = [\n",
    "        os.path.join(dir_path, filepath)\n",
    "        for filepath in os.listdir(dir_path)\n",
    "        if filepath.endswith(\".wav\")\n",
    "    ]\n",
    "    audio_paths += speaker_sample_paths\n",
    "    labels += [label] * len(speaker_sample_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f239517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampling rate of original audio tf.Tensor(16000, shape=(), dtype=int32)\n",
      "shape 1438464\n",
      "89\n"
     ]
    }
   ],
   "source": [
    "def load_noise_samples(path):\n",
    "    sample, sampling_rate = tf.audio.decode_wav(tf.io.read_file(path), desired_channels = 1)\n",
    "    print(\"sampling rate of original audio\", sampling_rate)\n",
    "    if sampling_rate == sample_rate:\n",
    "        print(\"shape\", sample.shape[0])\n",
    "        slices = int(sample.shape[0] / sample_rate)\n",
    "        print(slices)\n",
    "        sample = tf.split(sample[: slices * sample_rate], slices)\n",
    "        return sample\n",
    "    else:\n",
    "        print(\"sampling rate for \", path, \" is correct\")\n",
    "        return None\n",
    "    \n",
    "noises = []\n",
    "for path in noise_paths:\n",
    "    sample = load_noise_samples(path)\n",
    "    if sample:\n",
    "        noises.extend(sample)\n",
    "noises = tf.stack(noises)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0e3ba75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise(audio, noises = None, scale = 0.5):\n",
    "    if noises is not None:\n",
    "        tf_rnd = tf.random.uniform(\n",
    "            (tf.shape(audio)[0], ), 0, noises.shape[0], dtype = tf.int32    \n",
    "        )\n",
    "        noise = tf.gather(noises, tf_rnd, axis = 0)\n",
    "        \n",
    "        prop = tf.math.reduce_max(audio, axis = 1) / tf.math.reduce_max(noise, axis = 1)\n",
    "        prop = tf.repeat(tf.expand_dims(prop, axis = 1), tf.shape(audio)[1], axis = 1)\n",
    "        \n",
    "        audio = audio + noise * prop * scale\n",
    "        \n",
    "    return audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd8cb647",
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio_to_fft(audio):\n",
    "    audio = tf.squeeze(audio, axis = -1)\n",
    "    fft = tf.signal.fft(\n",
    "        tf.cast(tf.complex(real = audio, imag = tf.zeros_like(audio)), tf.complex64)\n",
    "    )\n",
    "    fft = tf.expand_dims(fft, axis = -1)\n",
    "    \n",
    "    return tf.math.abs(fft[:, : (audio.shape[1] // 2), :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99638617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
      "Speaker:\u001b[92m 6\u001b[0m\tPredicted:\u001b[92m 6\u001b[0m\n",
      "Speaker Predicted :  ahw_speaker\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-27 12:43:46.705139: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ahw_speaker'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = [\"data/audio/ahw_speaker/arctic_a0001.wav\"]\n",
    "labels = [\"unknown\"]\n",
    "predict(path, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2eec9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "50a65263",
   "metadata": {},
   "source": [
    "The GUI for recording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c1d4093f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import wave\n",
    "import time\n",
    "import threading\n",
    "import tkinter as tk\n",
    "import pyaudio\n",
    "\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3cedda04",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VoiceRecorder():\n",
    "    def __init__(self):\n",
    "        self.root = tk.Tk()\n",
    "        self.root.resizable(False, False)\n",
    "        self.button = tk.Button(text = \"recüî¥rd\", font = (\"Arial\", 120, \"bold\"), command = self.click_handler)\n",
    "        self.button.pack()\n",
    "        self.label = tk.Label(text = \"00:00:00\")\n",
    "        self.label.pack()\n",
    "        self.recording = False\n",
    "        self.root.mainloop()\n",
    "\n",
    "    def click_handler(self):\n",
    "        if self.recording:\n",
    "            self.recording = False\n",
    "            self.button.config(fg = \"black\")\n",
    "        else:\n",
    "            self.recording = True\n",
    "            self.button.config(fg = \"red\")\n",
    "            threading.Thread(target = self.record).start()\n",
    "\n",
    "    def record(self):\n",
    "        audio = pyaudio.PyAudio()\n",
    "        stream = audio.open(format = pyaudio.paInt16, channels = 1, rate = 16000, input = True, frames_per_buffer = 1024)\n",
    "        frames = []\n",
    "\n",
    "        start = time.time()\n",
    "\n",
    "        while self.recording:\n",
    "            data = stream.read(1024)\n",
    "            frames.append(data)\n",
    "\n",
    "            passed = time.time() - start\n",
    "            secs = passed%60\n",
    "            mins = passed // 60\n",
    "            hours = mins // 60\n",
    "            self.label.config(text = f\"{int(hours) : 02d}:{int(mins) : 02d}:{int(secs)  : 02d}\")\n",
    "\n",
    "        stream.stop_stream()\n",
    "        stream.close()\n",
    "        audio.terminate()\n",
    "\n",
    "        exists = True\n",
    "        global i\n",
    "        i = 1\n",
    "        while exists:\n",
    "            if os.path.exists(f\"data/Recordings/recording{i}.wav\"):\n",
    "                i += 1\n",
    "            else:\n",
    "                exists = False\n",
    "\n",
    "        sound_file = wave.open(f\"recording{i}.wav\", \"wb\")\n",
    "        sound_file.setnchannels(1)\n",
    "        sound_file.setsampwidth(audio.get_sample_size(pyaudio.paInt16))\n",
    "        sound_file.setframerate(16000)\n",
    "        sound_file.writeframes(b''.join(frames))\n",
    "        sound_file.close()\n",
    "        \n",
    "        shutil.move(f\"recording{i}.wav\", f\"data/Recordings/recording{i}.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2e3f2f8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.VoiceRecorder at 0x16c19a850>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VoiceRecorder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4c6f4c44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6209a967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
      "Speaker:\u001b[92m 2\u001b[0m\tPredicted:\u001b[92m 2\u001b[0m\n",
      "Speaker Predicted :  vishnu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-21 21:42:01.738109: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'vishnu'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = [f\"data/Recordings/recording{i}.wav\"]\n",
    "labels = [\"unknown\"]\n",
    "predict(path, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5fb90de8-663c-488c-8372-c367ccb89726",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openpyxl\n",
    "from openpyxl import Workbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2be191c9-a9df-4a4e-8509-050161079514",
   "metadata": {},
   "outputs": [],
   "source": [
    "workbook = Workbook()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37adc009-4738-4c93-aa4c-43ef673c7ce2",
   "metadata": {},
   "source": [
    "# Attendance loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "52158f8d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'i' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m student \u001b[38;5;129;01min\u001b[39;00m students:\n\u001b[1;32m      3\u001b[0m     VoiceRecorder()\n\u001b[0;32m----> 4\u001b[0m     path \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/Recordings/recording\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.wav\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      5\u001b[0m     labels \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munknown\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      6\u001b[0m     pred \u001b[38;5;241m=\u001b[39m predict(path, labels)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'i' is not defined"
     ]
    }
   ],
   "source": [
    "students = [\"vishnu\", \"ljm_speaker\", \"ahw_speaker\"]\n",
    "for student in students:\n",
    "    VoiceRecorder()\n",
    "    path = [f\"data/Recordings/recording{i}.wav\"]\n",
    "    labels = [\"unknown\"]\n",
    "    pred = predict(path, labels)\n",
    "    if pred == student:\n",
    "        print(\"Student \", student, \" is present\")\n",
    "    else:\n",
    "        print(\"Student \", student, \" is absent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9b93fd-0f39-4aa2-838e-8ffdbdbe25de",
   "metadata": {},
   "source": [
    "# Function to take Attendance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8540d322-6ead-4534-88ba-800585350374",
   "metadata": {},
   "outputs": [],
   "source": [
    "students = [\"vishnu\", \"ljm_speaker\", \"ahw_speaker\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f65792e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attendance_call():\n",
    "    global j\n",
    "    exists = True\n",
    "    j = 1\n",
    "    while exists:\n",
    "        if os.path.exists(f\"data/Attendance/class_attendance{j}.xlsx\"):\n",
    "            j+=1\n",
    "        else:\n",
    "            exists = False\n",
    "    workbook.save(filename = f\"class_attendance{j}.xlsx\")\n",
    "\n",
    "    sheet = workbook.active\n",
    "    student_name = sheet.cell(row = 1, column = 1)\n",
    "    student_name.value = \"Student Name\"\n",
    "    attendance = sheet.cell(row = 1, column = 2)\n",
    "    attendance.value = \"Present/Absent\"\n",
    "    proxy = sheet.cell(row = 1, column = 3)\n",
    "    proxy.value = \"Proxy\"\n",
    "\n",
    "    r = 2\n",
    "    for student in students:\n",
    "        name = sheet.cell(row = r, column = 1)\n",
    "        att = sheet.cell(row = r, column = 2)\n",
    "        prox = sheet.cell(row = r, column = 3)\n",
    "        name.value = student\n",
    "        \n",
    "        VoiceRecorder()\n",
    "        path = [f\"data/Recordings/recording{i}.wav\"]\n",
    "        labels = [\"unknown\"]\n",
    "        pred = predict(path, labels)\n",
    "        if pred == student:\n",
    "            att.value = \"Present\"\n",
    "            prox.value = \"None\"\n",
    "        else:\n",
    "            att.value = \"Absent\"\n",
    "            prox.value = pred\n",
    "        r += 1\n",
    "\n",
    "    workbook.save(f\"class_attendance{j}.xlsx\")\n",
    "\n",
    "    shutil.move(f\"class_attendance{j}.xlsx\", f\"data/Attendance/class_attendance{j}.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ca0a6a10-96a4-47c8-a207-f0beb800b49e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "Speaker:\u001b[92m 2\u001b[0m\tPredicted:\u001b[92m 2\u001b[0m\n",
      "Speaker Predicted :  vishnu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-27 12:45:52.949119: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "Speaker:\u001b[92m 2\u001b[0m\tPredicted:\u001b[92m 2\u001b[0m\n",
      "Speaker Predicted :  vishnu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-27 12:45:58.132642: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "Speaker:\u001b[92m 2\u001b[0m\tPredicted:\u001b[92m 2\u001b[0m\n",
      "Speaker Predicted :  vishnu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-27 12:46:02.564457: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "attendance_call()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d042f9-0b3e-4092-893c-46ebb1839ba7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
